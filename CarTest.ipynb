{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import airsim\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import datetime\n",
    "from PIL import Image\n",
    "\n",
    "import nbimporter\n",
    "import Classifier\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "\n",
    "depth_det = 10\n",
    "depth_cf = 40\n",
    "stacksize = 5\n",
    "\n",
    "#Customizable\n",
    "detectable = ['No_gesture', 'Swiping_Left', 'Swiping_Right', 'Stop_Sign', 'Pulling_Hand_In']\n",
    "target = ['Pulling_Hand_In'] #STOP SIGN TIME = 0.0215, REG=0.14\n",
    "ShowCropping = True\n",
    "CreateVideos = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCurrentFrame():\n",
    "    responses = client.simGetImages([\n",
    "        airsim.ImageRequest(\"1\", airsim.ImageType.Scene, False, False)])  #scene vision image in uncompressed RGB array\n",
    "\n",
    "    response = responses[0]\n",
    "    img1d = np.fromstring(response.image_data_uint8, dtype=np.uint8) # get numpy array\n",
    "    img_rgb = img1d.reshape(response.height, response.width, 3) # reshape array to 3 channel image array H X W X 3\n",
    "    #cv2.imwrite('fr.png',img_rgb)\n",
    "    \n",
    "    return img_rgb #as nparray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath, map_location='cpu')\n",
    "\n",
    "    my_dict = checkpoint['state_dict']\n",
    "    my_dict_copy = my_dict.copy()\n",
    "\n",
    "    for key in my_dict_copy.keys():\n",
    "        newkey = key[7:]\n",
    "        my_dict[newkey] = my_dict.pop(key)\n",
    "\n",
    "    checkpoint['state_dict'] = my_dict \n",
    "    \n",
    "    model = resnet.resnet10(\n",
    "        num_classes=2,\n",
    "        shortcut_type='B',\n",
    "        cardinality=32,\n",
    "        sample_size=112,\n",
    "        sample_duration=depth_det)\n",
    "\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTransforms():\n",
    "    scales = [1]\n",
    "    for i in range(1, 5):\n",
    "        scales.append(scales[-1] * 0.84089641525)\n",
    "        \n",
    "    norm_method = Normalize(get_mean(norm_value), [1, 1, 1])\n",
    "            \n",
    "    spatial_transform = spatial_transforms.Compose([\n",
    "        Scale(112),\n",
    "        CenterCrop(112),\n",
    "        ToTensor(norm_value), norm_method \n",
    "    ])\n",
    "\n",
    "    return spatial_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TURN THE 37 FRAMES FROM ./TESTVID INTO TENSOR INPUT <1,3,32,112,112>\n",
    "def TransformData(spatial_transform, frameST):\n",
    "    a = []\n",
    "    for frame in frameST:\n",
    "        image = torchvision.transforms.ToPILImage()(frame)\n",
    "        a.append(image)\n",
    "\n",
    "    spatial_transform.randomize_parameters()\n",
    "    input_spat = [spatial_transform(image) for image in a]\n",
    "    \n",
    "    im_dim = input_spat[0].size()[-2:]\n",
    "    clip = torch.cat(input_spat, 0).view((duration, -1) + im_dim).permute(1, 0, 2, 3)\n",
    "    input = clip[None, :, :, :, :] #Add 5th Dimension; batchSize=1\n",
    "    print(input.shape)\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CropHuman(frame, show=False):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    frame = cv2.resize(frame, (768, 288))\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    boxes, weights = hog.detectMultiScale(frame, winStride=(8,8) )\n",
    "    boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])\n",
    "    for (xA, yA, xB, yB) in boxes:\n",
    "        cv2.rectangle(frame, (xA, yA), (xB, yB),\n",
    "                        (0, 255, 0), 2)\n",
    "\n",
    "    if show:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(frame)#, cmap = 'gray', interpolation = 'bicubic')\n",
    "        plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "        plt.show()\n",
    "    \n",
    "    #return(boxes[0][0],boxes[0][1],boxes[0][2],boxes[0][3])\n",
    "    return(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_detector(fr, verbose=False):\n",
    "    cv2.imwrite('hum_det.png',fr)\n",
    "    boxes = CropHuman(fr, verbose)\n",
    "    if boxes.size == 0:\n",
    "        return 0,0,0,0\n",
    "    else:\n",
    "        x = boxes[0][0] * 2\n",
    "        y = boxes[0][1] * 2\n",
    "        xF = boxes[0][2] * 2\n",
    "        yF = boxes[0][3] * 2\n",
    "        return x,y,xF,yF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CropLastX(vstreamshort,x,y,xF,yF):\n",
    "    newstream = []\n",
    "    for fr in vstreamshort:\n",
    "        fr = fr[y:yF,x:xF]\n",
    "        height = fr.shape[0]\n",
    "        width = fr.shape[1]\n",
    "        fr = fr[int(round(height/7)):int(round(height/3)),int(round(width/9)):int(round(width*4/5))]\n",
    "        #fr = fr[int(round(height/14)):int(round(height/3.6)),int(round(width/9)):int(round(width*4/5))]\n",
    "        newstream.append(fr)\n",
    "    return newstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Detector(frameST):\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TransportError",
     "evalue": "Retry connection over the limit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTransportError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-01d7303df6cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#24to18lux\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mclient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mairsim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCarClient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfirmConnection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menableApiControl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcar_controls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mairsim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCarControls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jcruz\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\airsim\\client.py\u001b[0m in \u001b[0;36mconfirmConnection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconfirmConnection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Connected!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jcruz\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\airsim\\client.py\u001b[0m in \u001b[0;36mping\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ping'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetClientVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jcruz\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\msgpackrpc\\session.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, method, *args)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jcruz\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\msgpackrpc\\future.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRPCError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRPCError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTransportError\u001b[0m: Retry connection over the limit"
     ]
    }
   ],
   "source": [
    "try: #24to18lux\n",
    "    client = airsim.CarClient()\n",
    "    client.confirmConnection()\n",
    "    client.enableApiControl(True)\n",
    "    car_controls = airsim.CarControls()\n",
    "    \n",
    "    timingarr = []\n",
    "    percents = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    stopcalled = 0\n",
    "    comecalled = 0\n",
    "    rightcalled = 0\n",
    "    leftcalled = 0\n",
    "    ngcalled = 0\n",
    "    \n",
    "    Classifier.ClearDir('gesturevideos','avi')\n",
    "    start = timeit.default_timer()\n",
    "\n",
    "    videostream = []\n",
    "    while total < 2040:\n",
    "        try:\n",
    "            videostream.append(GetCurrentFrame())\n",
    "        except ValueError:\n",
    "            continue #skip to next frame\n",
    "        if len(videostream) >= depth_cf:\n",
    "            if len(videostream) % stacksize == 0: #every X frames run human_detector\n",
    "                total +=1\n",
    "                print('STACK ' + str(total))\n",
    "                x,y,xF,yF = human_detector(videostream[len(videostream)-1], ShowCropping)\n",
    "                if x or y or xF or yF: #Only continue if human detected\n",
    "                    #print('Human Detected.')\n",
    "                    croppedstream_recent = CropLastX(videostream[-depth_cf:],x,y,xF,yF) #Crop last X frames of VS\n",
    "                    exists = Detector(croppedstream_recent[-depth_det:]) #See if gesture detected\n",
    "                    if exists:\n",
    "                        classes, indices, percentages = Classifier.Classify(croppedstream_recent, CreateVideos)\n",
    "                        for i in range(0,26):\n",
    "                            gesture = classes[indices[0][i]]\n",
    "                            if gesture not in detectable:\n",
    "                                continue\n",
    "                            elif gesture in target:\n",
    "                                correct+=1\n",
    "                                percents.append(percentages[indices[0][i]].item())\n",
    "                                break\n",
    "                            else: #predicted a class, but wrong one\n",
    "                                if gesture == 'No_gesture':\n",
    "                                    ngcalled+=1\n",
    "                                if gesture == 'Swiping_Left':\n",
    "                                    leftcalled+=1\n",
    "                                if gesture == 'Swiping_Right':\n",
    "                                    rightcalled+=1\n",
    "                                if gesture == 'Stop_Sign':\n",
    "                                    stopcalled+=1\n",
    "                                if gesture == 'Pulling_Hand_In':\n",
    "                                    comecalled+=1\n",
    "                                break\n",
    "                #else:\n",
    "                    #print('Human NOT Detected')\n",
    "                print(\"Accuracy: \" + str(correct) + \"/\" + str(total))\n",
    "                print(\"Time per Frame (Avg): \" + str(sum(timingarr)/float(len(timingarr)+0.00000000000000000000000000000001)))\n",
    "                print(\"Percentage (Avg): \" + str(sum(percents)/float(len(percents)+0.00000000000000000000000000000001)))\n",
    "                print(ngcalled)\n",
    "                print(leftcalled)\n",
    "                print(rightcalled)\n",
    "                print(stopcalled)\n",
    "                print(comecalled)\n",
    "            end = timeit.default_timer()\n",
    "            timingarr.append(end-start) #time per frame taken\n",
    "            start = timeit.default_timer()   \n",
    "        \n",
    "    client.reset()\n",
    "    client.enableApiControl(False)\n",
    "except KeyboardInterrupt:\n",
    "    client.reset()\n",
    "    client.enableApiControl(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(timingarr)/len(timingarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
